import os

from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough, RunnableWithMessageHistory
from langchain_community.document_loaders import TextLoader
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_openai import ChatOpenAI
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_classic.retrievers import ContextualCompressionRetriever
from langchain_classic.retrievers.document_compressors import LLMChainExtractor
from langchain_classic.chains.combine_documents import create_stuff_documents_chain

# --------------------------
api_key = os.getenv("GPTSAPI_API_KEY")
base_url = os.getenv("GPTSAPI_BASE_URL")
llm = ChatOpenAI(
    api_key=api_key,
    base_url=base_url,
    model="gpt-3.5-turbo",  # æ¨è gpt-3.5-turbo/gpt-4ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰
    temperature=0.1,  # ReAct éœ€ä½æ¸©åº¦ï¼Œç¡®ä¿æ€è€ƒé€»è¾‘è¿è´¯
    timeout=30
)

# --------------------------
# 2. æ–‡æ¡£åŠ è½½ä¸åˆ†å‰²ï¼ˆæ ¸å¿ƒï¼šå¤„ç†é•¿æ–‡æ¡£ï¼‰
# --------------------------
# åŠ è½½æœ¬åœ°æ–‡æ¡£ï¼ˆç¤ºä¾‹ï¼štxt æ–‡ä»¶ï¼Œå¯æ›¿æ¢ä¸º PDFLoader/Docx2txtLoader ç­‰ï¼‰
loader = TextLoader("flower_knowledge.txt")  # æ–‡æ¡£è·¯å¾„ï¼šå­˜æ”¾é²œèŠ±çŸ¥è¯†ï¼ˆå¦‚å…»æŠ¤ã€å¯“æ„ç­‰ï¼‰
documents = loader.load()

# æ–‡æœ¬åˆ†å‰²ï¼šé€’å½’åˆ†å‰²ï¼ˆæŒ‰å­—ç¬¦é•¿åº¦æ‹†åˆ†ï¼Œä¿ç•™è¯­ä¹‰å®Œæ•´æ€§ï¼‰
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=300,  # æ¯ä¸ªç‰‡æ®µ 300 å­—ç¬¦
    chunk_overlap=50,  # ç‰‡æ®µé‡å  50 å­—ç¬¦ï¼ˆé¿å…è¯­ä¹‰æ–­è£‚ï¼‰
    length_function=len  # æŒ‰å­—ç¬¦æ•°è®¡ç®—é•¿åº¦
)
split_docs = text_splitter.split_documents(documents)
print(f"ğŸ“„ æ–‡æ¡£åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(split_docs)} ä¸ªæ–‡æœ¬ç‰‡æ®µ")

# --------------------------
# 3. æ„å»ºå‘é‡åº“ï¼ˆChroma + è½»é‡åµŒå…¥æ¨¡å‹ï¼‰
# --------------------------
# åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ï¼ˆall-MiniLM-L6-v2ï¼šè½»é‡ã€é«˜æ•ˆï¼Œé€‚åˆæœ¬åœ°è¿è¡Œï¼‰
embedding = SentenceTransformerEmbeddings(model_name="all-MiniLM-L6-v2")

# æ„å»º Chroma å‘é‡åº“ï¼ˆpersist_directory å¯é€‰ï¼šæŒä¹…åŒ–å‘é‡åº“ï¼Œä¸‹æ¬¡ç›´æ¥åŠ è½½ï¼‰
vector_db = Chroma.from_documents(
    documents=split_docs,
    embedding=embedding,
    persist_directory="./chroma_flower_db"  # å‘é‡åº“å­˜å‚¨è·¯å¾„
)
# vector_db.persist()  # æŒä¹…åŒ–å‘é‡åº“ï¼ˆé¿å…æ¯æ¬¡é‡æ–°æ„å»ºï¼‰

# --------------------------
# 4. æ„å»ºæ£€ç´¢å™¨ï¼ˆå¯é€‰ï¼šæ·»åŠ ä¸Šä¸‹æ–‡å‹ç¼©ï¼Œæå‡ç›¸å…³æ€§ï¼‰
# --------------------------
# åŸºç¡€æ£€ç´¢å™¨ï¼šä»å‘é‡åº“ä¸­æ£€ç´¢ top3 ç›¸å…³ç‰‡æ®µ
base_retriever = vector_db.as_retriever(search_kwargs={"k": 3})

# ä¸Šä¸‹æ–‡å‹ç¼©æ£€ç´¢å™¨ï¼ˆä¼˜åŒ–ï¼šç”¨ LLM è¿‡æ»¤æ— å…³ä¿¡æ¯ï¼Œæå‡æ£€ç´¢è´¨é‡ï¼‰
compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=base_retriever
)

# --------------------------
# 5. æ„å»º RAG é“¾ï¼ˆæœ€æ–°ç‰ˆï¼šcreate_retrieval_chain ç®€åŒ–é…ç½®ï¼‰
# --------------------------
# æç¤ºè¯æ¨¡æ¿ï¼ˆæ ¸å¿ƒï¼šå‘Šè¯‰ LLM åŸºäºæ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡å›ç­”ï¼Œé¿å…å¹»è§‰ï¼‰
prompt = ChatPromptTemplate.from_messages([
    ("system", """
    ä½ æ˜¯é²œèŠ±çŸ¥è¯†ä¸“å®¶ï¼Œä¸¥æ ¼åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¿¡æ¯å›ç­”ç”¨æˆ·é—®é¢˜ï¼š
    1. åªä½¿ç”¨ä¸Šä¸‹æ–‡æä¾›çš„ä¿¡æ¯ï¼Œä¸ç¼–é€ æœªæåŠçš„å†…å®¹ï¼›
    2. è‹¥ä¸Šä¸‹æ–‡æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥å›å¤â€œæŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³é²œèŠ±çŸ¥è¯†â€ï¼›
    3. å›ç­”ç®€æ´æ˜äº†ï¼Œåˆ†ç‚¹è¯´æ˜ï¼ˆå¦‚æœéœ€è¦ï¼‰ã€‚
    ä¸Šä¸‹æ–‡ï¼š{context}
    """),
    MessagesPlaceholder(variable_name="history", optional=True),  # å¯é€‰ï¼šæ”¯æŒå¯¹è¯å†å²
    ("human", "{input}")
])

# æ„å»ºã€Œæ–‡æ¡£æ•´åˆé“¾ã€ï¼šå°†æ£€ç´¢åˆ°çš„ç‰‡æ®µæ•´åˆä¸ºä¸Šä¸‹æ–‡
combine_docs_chain = create_stuff_documents_chain(llm, prompt)

# æ„å»ºå®Œæ•´ RAG é“¾ï¼šæ£€ç´¢ â†’ æ•´åˆ â†’ ç”Ÿæˆ
rag_chain = create_retrieval_chain(
    retriever=compression_retriever,  # ç”¨å‹ç¼©æ£€ç´¢å™¨ï¼ˆæˆ– base_retrieverï¼‰
    combine_docs_chain=combine_docs_chain
)

# --------------------------
# 6. å¯é€‰ï¼šæ·»åŠ å¯¹è¯å†å²ï¼ˆåŸºäº RunnableWithMessageHistoryï¼‰
# --------------------------
def get_session_history(session_id: str = "default") -> BaseChatMessageHistory:
    """å¤šä¼šè¯éš”ç¦»å­˜å‚¨ï¼ˆä¸´æ—¶ç”¨å­—å…¸ï¼Œå®é™…å¯æ›¿æ¢ä¸º Redis/MongoDBï¼‰"""
    if not hasattr(get_session_history, "session_store"):
        get_session_history.session_store = {}
    if session_id not in get_session_history.session_store:
        get_session_history.session_store[session_id] = InMemoryChatMessageHistory()
    return get_session_history.session_store[session_id]


# ç»‘å®šå¯¹è¯å†å²çš„ RAG é“¾ï¼ˆå¯ç”¨å¤šè½®å¯¹è¯éœ€ç”¨æ­¤é“¾ï¼‰
rag_chain_with_history = RunnableWithMessageHistory(
    runnable=rag_chain,
    get_session_history=lambda :get_session_history(),
    input_messages_key="input",
    history_messages_key="history",
    session_id_key="session_id"
)

# --------------------------
# 7. æµ‹è¯• RAG é“¾
# --------------------------
def test_rag():
    print("ğŸš€ é²œèŠ±çŸ¥è¯† RAG åŠ©æ‰‹ï¼ˆè¾“å…¥ 'quit' é€€å‡ºï¼‰")
    current_session_id = "user_001"  # ä¼šè¯ IDï¼ˆå¤šç”¨æˆ·æ—¶å¯åŠ¨æ€åˆ†é…ï¼‰

    while True:
        user_input = input("\nä½ ï¼š")
        if user_input.lower() == "quit":
            print("ğŸ‘‹ å†è§ï¼")
            break

        # æ‰§è¡Œ RAG é“¾ï¼ˆå¯ç”¨å¯¹è¯å†å²ç”¨ rag_chain_with_historyï¼‰
        result = rag_chain_with_history.invoke(
            input={"input": user_input},
            config={"configurable": {"session_id": current_session_id}}
        )

        # è¾“å‡ºç»“æœï¼ˆresult åŒ…å« answer å’Œ contextï¼Œå¯æŒ‰éœ€æ‰“å°ï¼‰
        print(f"åŠ©æ‰‹ï¼š{result['answer']}")

        # å¯é€‰ï¼šæ‰“å°æ£€ç´¢åˆ°çš„ç›¸å…³ä¸Šä¸‹æ–‡ï¼ˆè°ƒè¯•ç”¨ï¼‰
        # print("\nğŸ“Œ æ£€ç´¢åˆ°çš„ç›¸å…³ä¿¡æ¯ï¼š")
        # for i, doc in enumerate(result['context']['documents'], 1):
        #     print(f"{i}. {doc.page_content[:100]}...")


if __name__ == "__main__":
    test_rag()